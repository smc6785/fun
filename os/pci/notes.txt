# PCIE
- Peripherial Component Interconnect Exoress
- A PCIe lane is a serial connection for data transfer.
  - PCIe x4
  - PCIe x8
  - PCIe x16
  - PCIe 1.0, PCIe 2.0, PCIe 4.0 would have different lane speed.
- lspci -n
  - PCI bus address
    - Bus number
    - Device number
    - Function number
  - Class code
  - Vendor and device ID
- lspci -v
  - To see kernel module for each PICE device.
- lspci -x
  - To show the whole pci configuration space.
- /sys/class/pci_bus
  - Linux kernel uses PCI enumeration to scan all buses.
  - A symblic for /sys/devices
  - Rescan, echo 1 | sudo tee /sys/bus/pci/rescan
  - pci_init() -> bus enumeration starting from bus0 ->
    register each pcie device with the kernl ->
    /sys/bus/pci/devices

# Transaction types
- I/O read or write, (only for x86, ARM only supports MMIO)
  used to transfer data via CPU I/O ports.
  - Read and write from cpu ports one byte at a time.
  - BAR = port number + reserved + type
- Memory read or write, MMIO
  - BAR = address + prefetechable + locatable + type
- Configuration read or write
  - Configuration access mechanism, CAM, legacy, CPU IO ports
  - Enhanced configuration access mechanism, ECAM, MMIO
- Messages
  - Message signaled interrupt, MSI
  - MSI-X

- Upstream is towards the root.
- Downstream is away from the root.

# PCIe Topology
- CPU <-> root complex <-> memory
              |
              v
  PCIe endpoint/switches/repearter/PCIe bridage

- In root complex
  - CPU <-> Bridge <- primary bus -> PCI virtual bridge 
    <-downstram port-> PCIe endpoint
- Endpoint 
  - Reuqester or completer of PCIe transcation.
  - Legacy endpoint
    - Supports PCI transactions to support backward compatability.
  - Native endpoint
    - Does not support the following PCI transactions
      - I/O transactions
      - 32 bit only memory access
      - Locaked tansactions.
- Switch
  - Logical assembly of multiple virtual PCI to PCI bridge devices.
  - Upstream port
  - Downstream port
  - PCIexpress, point to point, bus is not shared,
    so switch like topology is required.

- Root complex, an interface between CPU and memory and the reset of the PCIe.
- Repeater, signal conditioner
  - Reimers
  - Redrivers
- Endpoint
- PCIe control signals
  - PERST, fundamental reset.
  - WAKE
  - CLKREQ
  - REFCLk
  - RX detect
  - PCIe x8
  - Polling, link training 
  - Configuration
  - PCIe link equalization, presets.

- A PCIe controller will be integrated to the SOC.
  And connected to internal bus/NOC.

# PCIe Initialization
- Druing hardware initialization, each PCIe link is setup following a
  negotiation of lane width and frequency of operation.
- No firmware or OS required.
# PCIe Link
- Link sonsists of dual undirectional differential signal.
  Can be of 1,2,4,8,12,16,32 lanes.
- A data clock is embedded using an encoding scheme.
# PCIe Lane
- Set of differential signal pair
- One pair for transmission, one pair for receiver.
# Differential signaling
- Method for electrically transmitting information
  using two complementry signals.
- Noise immunity.

# PCIe Controller
- Each layer has Tx and Rx.
- Transaction layer
  - Transaction layer packets, TLPs.
- Data link layer
  - Middle layer, does link management, data integrity, error detection
    and correction.
- Physical layer
  - Contains all circuitory for interface
    - Driver and input buffer.
    - Parallel to serial and serial to paralled.
    - PLL, Impedance matching circuitry.
    - Encode/decoder, scrambller/descrambller.
  - Logical sub block.
  - Electrical sub block.
  - Differential analog signaling increases noise immunity.
  - Differential analog signaling decreases electomagnetic interface(EMI)
    since the return current is flowing in th e2nd signal trace.
  - Data scrambling spreads RF energy, resulting in less EMI.
  - Link training
    - Both peers negotiate
- Address mode:
  - Memory.
  - I/O
  - Configuration
  - Message, like power management related transcation and interrupts.

====
- PCI drivers "discover" PCI devices in a system via pci_regiser_driver()
- Linux driver -> Linux PCI driver
- Linux PCI driver init
  - Enables the device
  - Requst MMIO/IOP resources
    - IOP, input/output operations per second
      A perfformance metric that measures the number of read and 
      write operations a storage device can handle per seconnd.
  - Set the MDA mask size (both cohernent and streaming DMA)
    - Coherent DMA is always synchronized between CPU and device.
      It is suitable for control structres like descriptor rings.
    - Streaming DMA is used for data that flows in one direction.
  - Allocate and initilize shared control data(pci_alllocate_coherent())


====
- OS creates a memory mapped region, MMIO 
  and writes the system address into BAR of the device PCI configuration space.
- The corresponding kernel driver allocates from kernel pool
  and transmits the physical address through MMIO BAR into register space of PCI device.
- The PCI device can act as a bus master and send DMA transaction to the main memory,
  and sign an MSI interrupt once DMA is complete
 
- Linux kernel PCI implementation:
  drivers/pci 
- Driver developers:
  include/linux/pci.h
- The main PCI driver structure:
  struct pci_dev
- PCI driver entry point:
  sturtc pci_driver

- Two types of PCI interrupts:
  - INTx, pin based, shared.
  - MSI/MSI-X, never shared.
  - /proc/interrupts
  - echo 5 > /proc/irq/44/smp_affinity
    - Change cpu interrupt affinity IRQ 44 to first and third core
  - int pci_alloc_irq_vectors()
  - void pci_free_irq_vectors()

====
# PCI device driver skeleton
/* Sample Linux PCI device driver */

#include <linux/init.h>
#include <linux/module.h>
#include <linux/pci.h>

#define MY_DRIVER "my_pci_driver"

/* This sample driver supports device with VID = 0x010F, and PID = 0x0F0E*/
static struct pci_device_id my_driver_id_table[] = {
    { PCI_DEVICE(0x010F, 0x0F0E) },
    {0,}
};

MODULE_DEVICE_TABLE(pci, my_driver_id_table);

static int my_driver_probe(struct pci_dev *pdev, const struct pci_device_id *ent);
static void my_driver_remove(struct pci_dev *pdev);

/* Driver registration structure */
static struct pci_driver my_driver = {
    .name = MY_DRIVER,
    .id_table = my_driver_id_table,
    .probe = my_driver_probe,
    .remove = my_driver_remove
};

/* This is a "private" data structure */
/* You can store there any data that should be passed between driver's functions */
struct my_driver_priv {
    u8 __iomem *hwmem;
};

/* */

static int __init mypci_driver_init(void)
{
    /* Register new PCI driver */
    return pci_register_driver(&my_driver);
}

static void __exit mypci_driver_exit(void)
{
    /* Unregister */
    pci_unregister_driver(&my_driver);
}

void release_device(struct pci_dev *pdev)
{
    /* Disable IRQ #42*/
    free_irq(42, pdev);
    /* Free memory region */
    pci_release_region(pdev, pci_select_bars(pdev, IORESOURCE_MEM));
    /* And disable device */
    pci_disable_device(pdev);
}

/* */

static irqreturn_t irq_handler(int irq, void *cookie)
{
   (void) cookie;
   printk("Handle IRQ #%d\n", irq);
   return IRQ_HANDLED;
}

/* Reqest interrupt and setup handler */
int set_interrupts(struct pci_dev *pdev)
{
    /* We want MSI interrupt, 3 lines (just an example) */
    int ret = pci_alloc_irq_vectors(pdev, 3, 3, PCI_IRQ_MSI);

    if (ret < 0) {
        return ret;
    }

    /* Request IRQ #42 */
    return request_threaded_irq(42, irq_handler, NULL, 0, "TEST IRQ", pdev);
}

/* Write some data to the device */
void write_sample_data(struct pci_dev *pdev)
{
    int data_to_write = 0xDEADBEEF; /* Just a random trash */

    struct my_driver_priv *drv_priv = (struct my_driver_priv *) pci_get_drvdata(pdev);

    if (!drv_priv) {
        return;
    }

    /* Write 32-bit data to the device memory */
    iowrite32(data_to_write, drv_priv->hwmem);
}

/* This function is called by the kernel */
static int my_driver_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
{
    int bar, err;
    u16 vendor, device;
    unsigned long mmio_start,mmio_len;
    struct my_driver_priv *drv_priv;

    /* Let's read data from the PCI device configuration registers */
    pci_read_config_word(pdev, PCI_VENDOR_ID, &vendor);
    pci_read_config_word(pdev, PCI_DEVICE_ID, &device);

    printk(KERN_INFO "Device vid: 0x%X pid: 0x%X\n", vendor, device);

    /* Request IO BAR */
    bar = pci_select_bars(pdev, IORESOURCE_MEM);

    /* Enable device memory */
    err = pci_enable_device_mem(pdev);

    if (err) {
        return err;
    }

    /* Request memory region for the BAR */
    err = pci_request_region(pdev, bar, MY_DRIVER);

    if (err) {
        pci_disable_device(pdev);
        return err;
    }

    /* Get start and stop memory offsets */
    mmio_start = pci_resource_start(pdev, 0);
    mmio_len = pci_resource_len(pdev, 0);

    /* Allocate memory for the driver private data */
    drv_priv = kzalloc(sizeof(struct my_driver_priv), GFP_KERNEL);

    if (!drv_priv) {
        release_device(pdev);
        return -ENOMEM;
    }

    /* Remap BAR to the local pointer */
    drv_priv->hwmem = ioremap(mmio_start, mmio_len);

    if (!drv_priv->hwmem) {
       release_device(pdev);
       return -EIO;
    }

    /* Set driver private data */
    /* Now we can access mapped "hwmem" from the any driver's function */
    pci_set_drvdata(pdev, drv_priv);

    write_sample_data(pdev);

    return set_interrupts(pdev);
}

/* Clean up */
static void my_driver_remove(struct pci_dev *pdev)
{
    struct my_driver_priv *drv_priv = pci_get_drvdata(pdev);

    if (drv_priv) {
        if (drv_priv->hwmem) {
            iounmap(drv_priv->hwmem);
        }

        pci_free_irq_vectors(pdev);

        kfree(drv_priv);
    }

    release_device(pdev);
}

MODULE_LICENSE("GPL");
MODULE_AUTHOR("Oleg Kutkov <contact@olegkutkov.me>");
MODULE_DESCRIPTION("Test PCI driver");
MODULE_VERSION("0.1");

module_init(mypci_driver_init);
module_exit(mypci_driver_exit);

====
- PCI Root Complex
- 8 bus. (3 bits)
- Each bus can have up to 32 devices. (5 bits)
- Each devices can have up to 8 functions. (3 bits)

- Type0 configuration request to discover other end-point pci devices
- Read and wrtie of these request are done in ECAM.

====
# Root complex driver for PCI subsystem.
- Root complex
  - Configuration space
    - Type0/1 64 bit standard headers
  - IP registers, used to configure root complex IP
    - Address translation unit/table:
      - Translates CPU address to PCI address.
      - When cpu accesses some address in root complex pci address space,
        Address translation unit will translate that to ECAM address, 
        which would be some address in PCI endpoint address space.  
  - Configuration address space, used by CPU to address PCI address space
    - PCI address space:
      - End point device address space:
        - Configuration space
        - Memory space
- In order for the host system to read a particular PCI end-point's 
  configuration space. It has to be mapped to PCI address space.
  - ECAM, Enhanced configuration access mechanism.
    - Bus number + device number + function number
  - PCI uses ECAM to allocate the end-point's configuration space to 
    PCI address space in root complex.
  - All PCI end-points devices address space would get mapped to root
    complex configuration address space in order for cpu to access.
  - Host write all one to endpoint's BAR to get the size.
  - Host write cpu address value to endpoint's BAR 
    using root complex address translation unit(ECAM)
- PCI end-point write/read to host memory:
  - Host system should make end-point to a bus master
    - By setting a bit in PCI end-point's config space
- Interrupt
  - Legacy
    - Assert and deassert INTx by writing to config space
  - MSI
    - Write message to some address in config space
      - Root complex will signal the interrupt
- Device tree is used for devices that are not enumerated dynamically,
  like root complex.

- drivers/pci/host
  - Each platform's unique root complex driver.
  - These drivers will be probed based on devices tree node.
  - Will initialize the root complex IP,
    clocks, program PHY layer, program reset lines, 
    program the address translatio unit in root complex,
    extract the io resource information and
    memory resouce informationn from the ranges property,
    and invoke an API to start PCI enumeration process.
- pci root complex drier <-> pci core <-> pci device drvier
- pci core uses pci root complex driver API to start PCI enumeration,
  reading pci device's config space.
- pci device drivers will register themselves with pci core.
- pci core will scan the pci bus, and whenever it finds a device, it
  will probe the corresponding pci device drivers.
- These pci device drivers will have information abouut the pci device
  and implement interrupt handlers and interact with 
  domain specific upper lay (ethernet, USB).

========
# VFIO
- Virtual function IO
- A linux kernel subsystem that provides a framework for
  exposing direct device acess to userspace app and virtual mahcine.
- ARM MMU recap:
  - EL0 user space   --(VA)--> stage 1 MMU                  
                     --(IPA, intermediate PA)--> stage2 MMU --(PA)--> memory
  - EL1 kernel space --(VA)--> stage 1 MMU (S1 page tables) 
                     --(IPA)--> stage2 MMU --(PA)--> memory
  - EL2 hypervisor   ----> stage 2 MMU (S2 page tbales) ----> memory

- DMA
  - Device -> IOMMU/SMMU <- IOTLBs
                  |          |
  - ( Physical memory (IOMMU page table))


