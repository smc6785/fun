cpu, serial

gpu, parallel
- VDI, virtual desktop infrastructure
  - graphic intensive applicaions
- AI
- HPC, high performance computing

GPU code is called kernel
CPU launches kernels on device

NVIDIA, CUDA, compute unified device architecture
- one program for CPU and GPU
- compiler generates code for each

CPU and GPU has separate DRAM memories
Moving data from CPU to GPU and GPU back to CPU.
Allocating GPU memory

cudaMalloc, CPU allocates GPU memory
cudaMemcpy, CPU copies data to GPU
CPU launches kernel on GPU, parallelism expressed here
cudaMemcpy, CPU copies results from GPU

========
# gpu are SIMD -- single instruction mulitple data
- DSP
  - ADD, MUL
  - x[3] (not a load)
  - x[y] (load, where u depends on the data)
- GPU
  - ADD, MUL, LOAD, STORE
- CPU
  - ADD, MUL, LOAD, STORE, COMPARE, BRANCH

WGP, work group processor = 2 CU, compute unit
warp (signle instruction across 32)
in lockstep, group of cores executing the same instruction

========
# openGL, open Graphics Library
- the API is typically used to interact with a graphics processing unit to achieve hw acceletrated rendering
========
# openCl, open computing language
- kernel
  - basic unit of executable code, similiar to a C function
  - Data-parallel or task-parallel
- program
  - collection of kernels and other fucntions
  - analogous to a dynamic library
- applications queue kernel execution instances
  - queued in-order
  - executed in order or out of order  

- kernel executed across a global domain of work-items
  - glocal dimensions define the range of computation
  - one work-item pro computation, executed in parallel
- work items are grouped in local workgroups
  - local dimensions define the size of the workgroups
  - executed together on one device
  - share local memory and synchronization
- caveats
  - global work-items musy be independent: No global synchronization
  - synchronization can be doen within a workgroup
- e.g. 
  - 2D, global_dim[3] = {1024,1024,1}
  - global dimensions: 1024*1024, whole problem space
  - local dimensions: 128*128, executed together
- No global synchronization, only within workgroups
- the work-items in each workgroup can 
  - use barriers to synchronize execution
  - use memory fences to synchonize memory accesses

- openCL platform model
  - a host connected to one or more openCL devices
    - gpu, dps, more cpu cores
    - a compute unit composed of one or more processing elements which execute code as SIMD or SPMD
- openCL memory model
  - host memory, on the CPU
  - local global/constant memory, not synchronized
  - local memory, shared within a workgroup
  - private memory, per work-item
- openCL objects
  - setup
    - devices, gpu, cpu, openCL device
    - contexts, collection of device
    - queues, submit work to the device
  - memory
    - buffers, blocks of memory
    - images, 2D or 3D formatted images
  - execution
    - programs, collections of kernels
    - kernels, argument/execution instances
  - synchronization/profiling
    - events

========

 
