#include <stdio.h>
#include <stdint.h>
#include <assert.h>
#include <time.h>
#include <math.h>

#include <immintrin.h>

#define N 1024
#define BLOCK 8

float __attribute__ ((aligned(64))) A[N][N];
float __attribute__ ((aligned(64))) B[N][N];
float __attribute__ ((aligned(64))) C[N][N];
float __attribute__ ((aligned(64))) val[N][N];

// __m256 data type from intel cpu instrinsics guide.
__m256 *Am = (__m256*)A;
__m256 *Bm = (__m256*)B;
__m256 *Cm = (__m256*)C;

uint64_t nanos(){
  struct timespec start;
  clock_gettime(CLOCK_MONOTONIC_RAW, &start);
  return (uint64_t)start.tv_sec*1000000000 + (uint64_t)start.tv_nsec;
}

void matmul(){
  for (int by = 0; by < N; by += BLOCK){
    for (int bx = 0; bx < N; bx += BLOCK){

      // compute
      float tc [BLOCK][BLOCK];
      for(int y = 0; y< BLOCK; y++){
        for(int x = 0; x< BLOCK; x++){
          float acc = 0; 
          for(int k = 0; k< N; k++){
            /* 
               If B is transposed, we can do 
               acc += A[by+y][k] * B[bx+x][k];
               which is faster.
            */
            acc += A[by+y][k] * B[k][bx+x];
          }
          tc[y][x] = acc;
        }
      }

      // store
      for (int y = 0; y<BLOCK; y++){
        for (int x = 0; x<BLOCK; x++){
          C[by+y][bx+x] = tc[y][x];
        }
      }

#if 0
      __m256 tc[BLOCK][BLOCK];
      for (int y = 0; y < BLOCK; y++){
        for (int x = 0; x < BLOCK; x++){
          __m256 tmp = {};
          for (int k = 0; k < BLOCK; k++){
            tmp = _mm256_fmadd_ps(
              Am[((by+y)*N + k)/8],
              Am[((bx+x)*N + k)/8], tmp);
          }
          __m256 ftmp = {};
          for (int i = 0; i < 8; i++) ftmp +=tmp[1];
          tc[y][x] = ftmp;
        }
      }

      for (int y = 0; y < BLOCK; y++){
        for (int x = 0; x < BLOCK; x++){
          Cm[(by+y)*N + bx +x] = tc[x][y];
        }
      }
#endif 
    }
  }
}

int main(){
  assert(N%BLOCK == 0);
  uint64_t start = nanos();

  // Read file generated by gemm.py to test gemm.c
  // xxd matmul
  FILE *f = fopen("./matmul", "rb");
  fread(A,1,sizeof(float)*N*N, f);
  fread(B,1,sizeof(float)*N*N, f);
  fread(val,1,sizeof(float)*N*N, f);
  fclose(f);

  matmul();

  uint64_t end = nanos();
  // double tflop = (2.0*N*N*N)*1e-12;
  double gflop = (2.0*N*N*N)*1e-9;
  double s = (end-start)*1e-9;
  printf("%f GFLOP/S\n", gflop/s);
  

  for (int y = 0;y < N; y++){
    for (int x = 0;x < N; x++){
      // if(C[y][x] != val [y][x]){
      if(fabsf(C[y][x] - val [y][x]) > 1e-3){
        printf("MISMATCH AT %d x %d, %f != %f\n", y, x, C[y][x], val[y][x]);
        return -1;
      }
    }
  }
  printf("match\n");

  return 0;
}

#if 0
gcc -O2 makes it faster.
gcc -march=native, tells the compiler to use the current host's instruction set.

local cache
cache aware algorithem

/proc/cpuinfo on a MAC:
sysctl -a | grep machdep.cpu | less
Intel(R) Core(TM) i5-7360U CPU @ 2.30GHz
Kaby Lake microarchitecture
Kaby Lake is Intel's codename for its seventh generation Core microprocessor family.

Use the below to get FMA operation on x86.
https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#ig_expand=3079,3103,3107&avxnewtechs=FMA

It supports FMA3.
FMA instruction set is an extension to the 128 and 256 bit Streaming
  SIMD extensions instruction in x86 to perform fused multiply-add (FMA)
  operations.
Fused multiply-add is a floating point multiply-add operation performed
  in one step(fused operation).

When "gcc -mavx" is used, gcc built-in AVX functions are enabled.
AVX, advanced vector extension is the SIMD extension for x86.
AVX2 is only for integers.

// Simple gemm in C 
for(int x = 0; x< N; x++){
  for(int y = 0; y< N; y++){
    float acc = 0; 
    for(int z = 0; z< N; z++){
      acc += A[x][z] * B[z][y];
    }
    C[x][y] = acc;
  }
}

./gemm.py && gcc -march=native -O2 gemm.c && ./a.out

BLAS, basic linear allgebra subporgrams

print &A
x/32wx 0x12345678

#endif
